{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMxmiUP5fhS76STLU/blyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirazkk/Generative_Ai_Projects/blob/main/Project_2_LangChain_RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **LangChain RAG with Google Gemini Flash and Pinecone**\n",
        "\n",
        "## **Overview**\n",
        "This project implements a **Retrieval-Augmented Generation (RAG)** system using **LangChain**, **Google Gemini Flash**, and **Pinecone**. The system retrieves relevant information from a document store and generates contextual responses using an AI model. The key components include:\n",
        "1. Storing vectorized documents in **Pinecone**.\n",
        "2. Using **Google Gemini Flash** to generate embeddings and responses.\n",
        "3. Integrating **LangChain** to build the RAG workflow.\n",
        "\n",
        "## **Prerequisites**\n",
        "Before you begin, ensure you have the following:\n",
        "\n",
        "1. **Python 3.8+** installed.\n",
        "2. **API keys** for:\n",
        "   - Google Gemini Flash (Gemini-1.5-flash)\n",
        "   - Pinecone\n",
        "\n",
        "3. Install the following Python libraries:\n",
        "  \n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QRogio3WndCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pinecone-client google-generativeai python-dotenv"
      ],
      "metadata": {
        "id": "fRUqD-zJBTEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "83657eea-cb3d-4706-b342-e927fe0dcde5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Set Up Environment Variables**\n",
        "To securely manage API keys, store them in a `.env` file in your project directory. The `.env` file should look like this:\n",
        "\n",
        "```\n",
        "PINECONE_API_KEY=your_pinecone_api_key\n",
        "PINECONE_ENVIRONMENT=your_pinecone_environment\n",
        "GOOGLE_API_KEY=your_google_api_key\n",
        "```\n",
        "\n",
        "Next, use the following script to load these environment variables:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "siaaDsCXpE1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "id": "-9aMLLWoGJIl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Step 2: Initialize Pinecone**\n",
        "Pinecone is used to store and retrieve vectorized documents. The following code sets up Pinecone and creates an index for storing your documents:"
      ],
      "metadata": {
        "id": "BLku1rBZrgLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "import os\n",
        "\n",
        "# Initialize Pinecone using Pinecone class\n",
        "pinecone_client = pinecone.Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENVIRONMENT\"])\n",
        "\n",
        "# Create or connect to an index using the pinecone_client instance\n",
        "index_name = \"new-rag-project\"\n",
        "if index_name not in pinecone_client.list_indexes().names(): # Use pinecone_client to access methods\n",
        "    pinecone_client.create_index(index_name, dimension=768)  # Adjust based on embedding size\n",
        "\n",
        "index = pinecone_client.Index(index_name) # Use pinecone_client to access methods"
      ],
      "metadata": {
        "id": "UEtpaXJyEChj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Use LangChain for RAG Workflow**\n",
        "\n",
        "### **1. Set Up Embedding Model**\n",
        "To convert your documents into vector embeddings, we use the Google Generative AI Embedding Model (embedding-001). This model creates numerical vector representations of text, which are later stored in Pinecone for efficient retrieval.\n"
      ],
      "metadata": {
        "id": "1EZLAMLar7IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "j-m-w8XfGy33"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Set Up Document Loader**\n",
        "Load and preprocess the documents you wish to index."
      ],
      "metadata": {
        "id": "wt8R5nKRsi8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents\n",
        "loader = TextLoader(\"/content/schedule of web development.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "HFkaTYV1KUzt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Embed and Store Documents in Pinecone**\n",
        "Now, embed the document chunks and store them in Pinecone:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4p_CgQPbs1A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "for doc in tqdm(docs):\n",
        "    # Generate the vector (embedding)\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "\n",
        "    # Ensure metadata is a dictionary\n",
        "    metadata = {\n",
        "    \"text\": doc.page_content,  # The actual content to be embedded\n",
        "    \"source\": doc.metadata[\"source\"]  # Metadata like source\n",
        "    }\n",
        "\n",
        "    # Upsert the document into Pinecone\n",
        "    index.upsert([(doc.metadata[\"source\"], vector, metadata)])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-KY1xQSLdhz",
        "outputId": "0466a43a-c727-45ad-f5fb-91f94a3e49cf"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  3.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Set Up Retriever**\n",
        "Now, set up the Pinecone retriever using LangChain:\n"
      ],
      "metadata": {
        "id": "3vRWDH2ws9aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "# Initialize Pinecone with index, text_key, and the Embeddings object\n",
        "retriever = Pinecone(index, embedding=embeddings, text_key=\"text\")\n",
        "\n",
        "# Use as_retriever (no need for search_kwargs now)\n",
        "retriever = retriever.as_retriever()"
      ],
      "metadata": {
        "id": "MJIO5RHRVedT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Set Up Google Gemini Flash Chat Model**\n",
        "\n",
        "To generate responses, we use the `gemini-1.5-flash ` model from Google Gemini Flash. This chat model processes user queries and combines them with context retrieved from Pinecone to generate relevant and human-like responses.\n"
      ],
      "metadata": {
        "id": "Zb5nTSN6tDwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Google chat model\n",
        "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
        "\n",
        "# Access the API key using os.environ\n",
        "google_gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model name, e.g., 'gemini-1.5-flash'\n",
        "    api_key=GOOGLE_API_KEY, #Use the environment variable for the API key\n",
        "    temperature=0.8,  # controls the randomness of the responses generated by the model.\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z2l5uzmOWlBN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Step 5: Combine Retriever and LLM**\n",
        "\n",
        "Integrate the **retriever** and **Google Gemini Flash** using LangChain’s **RetrievalQA** chain:\n",
        "\n",
        "*   Retriever: Finds the most relevant document chunks based on the query.\n",
        "\n",
        "*  LLM `(gemini-1.5-flash):` Uses the query and retrieved documents to generate a context-aware response.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VuViG46IthAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores.pinecone import Pinecone\n",
        "from langchain.chains.retrieval_qa.base import BaseRetrievalQA\n",
        "\n",
        "# Setup RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=google_gemini,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever  # Use as_retriever() to get a BaseRetriever\n",
        ")"
      ],
      "metadata": {
        "id": "ZtXLg95JfpOp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Step 6: Query the RAG System**\n",
        "\n",
        "Test the system by asking a question, and the RAG pipeline will retrieve relevant documents and generate a response:\n"
      ],
      "metadata": {
        "id": "MpTKBmbxtoMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"give me intro of html\"\n",
        "response = qa_chain.run(query)\n",
        "\n",
        "print(\"Question:\", query)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wiQYO8Gm850",
        "outputId": "7f92e2d4-37a6-4c25-cf8f-f1dd9794c791"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: give me intro of html\n",
            "Response: HTML stands for HyperText Markup Language.  It's the foundation of all web pages.  You use HTML tags to structure content like headings, paragraphs, images, and links.  Browsers interpret these tags to display the content correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion**\n",
        "This project demonstrates how to create a **Retrieval-Augmented Generation (RAG)** system using **LangChain**, **Google Gemini Flash**, and **Pinecone**. The system efficiently retrieves relevant documents from Pinecone based on a user's query and uses a `gemini-1.5-flash` model to generate contextual responses.\n",
        "\n",
        "By following the steps outlined in this documentation, you can set up a similar RAG system for your use cases and integrate it with various models and data sources."
      ],
      "metadata": {
        "id": "6Z3HEuLZxaic"
      }
    }
  ]
}